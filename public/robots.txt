# Standard robots.txt - Allow all, block query strings and private resources
User-agent: *
Allow: /
Disallow: /node_modules/
Disallow: /src/
Disallow: /?*
Disallow: /*.json$
Disallow: /*.log$
Disallow: /admin/
Disallow: /private/
Disallow: /backup/
Disallow: /temp/

# Block known malicious crawlers
User-agent: sqlmap
Disallow: /

User-agent: Nikto
Disallow: /

User-agent: w3af
Disallow: /

User-agent: Nessus
Disallow: /

User-agent: OpenVAS
Disallow: /

# Sitemap location for search engines
Sitemap: https://dhirendrasinghdhami.com.np/sitemap.xml

# Clean-param directives tell Google to treat these URLs as equivalent
Clean-param: utm_source /
Clean-param: utm_medium /
Clean-param: utm_campaign /
Clean-param: fbclid /
Clean-param: gclid /

# Crawl-delay to be respectful to servers
Crawl-delay: 1